{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clitics = [('كن', 'كم'),\n",
    "           ('هم', 'هن'),\n",
    "           ('كي', 'ك'),\n",
    "           ('ه', 'ها')]\n",
    "\n",
    "all_clitics = ['كم',\n",
    "               'كن',\n",
    "               'ك',\n",
    "               'كي',\n",
    "               'ه',\n",
    "               'ها',\n",
    "               'هم',\n",
    "               'هن']\n",
    "\n",
    "clitics_genders = {'كم': 'M',\n",
    "                   'كن': 'F',\n",
    "                   'ك':  'M',\n",
    "                   'كي': 'F',\n",
    "                   'ه':  'M',\n",
    "                   'ها': 'F',\n",
    "                   'هم': 'M',\n",
    "                   'هن': 'F'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(path, data):\n",
    "    with open(path, mode='w') as f:\n",
    "        for ex in data:\n",
    "            for i, (token, tag) in enumerate(zip(ex.tokens, ex.tags)):\n",
    "                f.write(ex.id + ' ' + token + ' ' + tag)\n",
    "                f.write('\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLevalInfo:\n",
    "    \"\"\"\n",
    "    Simple object to save sentence info\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, tags, sent_label, id):\n",
    "        self.tokens = tokens\n",
    "        self.tags = tags\n",
    "        self.sent_label = sent_label\n",
    "        self.id = id\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "\n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_original_and_reinflections(data):\n",
    "    \"\"\"\n",
    "    This function takes advantage of the way I created the data.\n",
    "    It groups the original and reinflection sentences together.\n",
    "    \n",
    "    If the id contains a '.' (.1, .2, .3), we know that this sentence is a\n",
    "    reinflection, so we simply add it with its sisters sentences\n",
    "\n",
    "    Args:\n",
    "        - data: list of strings\n",
    "    \n",
    "    Returns:\n",
    "        - all_org_and_re_ids: list of list. Each sublist contains the original \n",
    "                          and reinflection ids (when applicable)\n",
    "                          \n",
    "        - all_org_and_re_english: list of list of english sentences.\n",
    "                                 \n",
    "        - all_org_and_re_arabic: list of list. Each sublist contains the original \n",
    "                          and reinflection sentences (when applicable)\n",
    "        \n",
    "         - all_org_and_re_labels: list of list. Each sublist contains the original \n",
    "                          and reinflection labels (when applicable)\n",
    "    \n",
    "    \"\"\"\n",
    "    all_org_and_re_ids = []\n",
    "    all_org_and_re_english = []\n",
    "    all_org_and_re_arabic = []\n",
    "    all_org_and_re_labels = []\n",
    "\n",
    "    for ex in data:\n",
    "        id = ex[0].strip()\n",
    "        english = ex[1].strip()\n",
    "        arabic = ex[2].strip()\n",
    "        # replace <s> </s>\n",
    "        arabic = arabic.replace('<s>','').replace('</s>','')\n",
    "        label = ex[3].strip()\n",
    "    \n",
    "        if '.' not in id:\n",
    "            all_org_and_re_ids.append([id])\n",
    "            all_org_and_re_english.append([english])\n",
    "            all_org_and_re_arabic.append([arabic])\n",
    "            all_org_and_re_labels.append([label])\n",
    "        else:\n",
    "            all_org_and_re_ids[-1].append(id)\n",
    "            all_org_and_re_english[-1].append(english)\n",
    "            all_org_and_re_arabic[-1].append(arabic)\n",
    "            all_org_and_re_labels[-1].append(label)\n",
    "    \n",
    "    return all_org_and_re_ids, all_org_and_re_english, all_org_and_re_arabic, all_org_and_re_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_input_corpus(dataset_path, split='train'):\n",
    "    ids = read_data(dataset_path+split+'.ids')\n",
    "    en = read_data(dataset_path+split+'.en')\n",
    "    ar = read_data(dataset_path+split+'.arin')\n",
    "    labels = read_data(dataset_path+split+'.arin.label')\n",
    "    \n",
    "    # Grouping\n",
    "    grouped = group_original_and_reinflections(zip(ids, en, ar, labels))\n",
    "    \n",
    "    grouped_ids, grouped_en = grouped[0], grouped[1]\n",
    "    grouped_ar, grouped_labels = grouped[2], grouped[3]\n",
    "    \n",
    "    # Tagging\n",
    "    tokens, tags, all_data = tag_tokens(grouped_ids, grouped_ar, grouped_labels)\n",
    "    \n",
    "    assert len(tokens) == len(tags) == len(all_data)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    print(len(grouped))\n",
    "    return tokens, tags, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_tokens(grouped_ids, grouped_ar, grouped_labels):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    tagged_data = []\n",
    "\n",
    "    for ids, ar_sents, labels in zip(grouped_ids, grouped_ar, grouped_labels): \n",
    "        if len(ids) == 1: # BB case, label each token as B\n",
    "            ar_tokens = [t for t in ar_sents[0].split()]\n",
    "            tokens.append(ar_tokens)\n",
    "            tags.append(['B+B']*len(ar_tokens))\n",
    "            \n",
    "            tagged_data.append(TokenLevalInfo(tokens=ar_tokens,\n",
    "                                              tags=['B+B']*len(ar_tokens),\n",
    "                                              sent_label=labels[0],\n",
    "                                              id=ids[0]))\n",
    "\n",
    "        elif len(ids) == 2: # MB/FB or BM/BF cases\n",
    "            sent_1_tokens = ar_sents[0].split()\n",
    "            sent_1_label = labels[0]\n",
    "            sent_2_tokens = ar_sents[1].split()\n",
    "            sent_2_label = labels[1]\n",
    "            \n",
    "            # getting the tags\n",
    "            sent_1_tags, sent_2_tags = tag_two(sent_1_tokens=sent_1_tokens,\n",
    "                                               sent_1_label=sent_1_label,\n",
    "                                               sent_2_tokens=sent_2_tokens,\n",
    "                                               sent_2_label=sent_2_label)\n",
    "            \n",
    "            # adding to tokens and tags\n",
    "            tokens.append(sent_1_tokens)\n",
    "            tags.append(sent_1_tags)\n",
    "    \n",
    "            tokens.append(sent_2_tokens)\n",
    "            tags.append(sent_2_tags)\n",
    "            \n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_1_tokens,\n",
    "                                  tags=sent_1_tags,\n",
    "                                  sent_label=sent_1_label,\n",
    "                                  id=ids[0]))\n",
    "\n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_2_tokens,\n",
    "                                  tags=sent_2_tags,\n",
    "                                  sent_label=sent_2_label,\n",
    "                                  id=ids[1]))\n",
    "            \n",
    "            # checking\n",
    "            if sent_1_tags.count('B+B') != sent_2_tags.count('B+B'):\n",
    "                print(ids)\n",
    "            \n",
    "            if sent_1_tags.count('1M+B') != sent_2_tags.count('1F+B'):\n",
    "                print(ids)\n",
    "            \n",
    "            if sent_1_tags.count('2M+B') != sent_2_tags.count('2F+B'):\n",
    "                print(ids)\n",
    "                \n",
    "            if sent_1_tags.count('B+M') != sent_2_tags.count('B+F'):\n",
    "                print(ids)\n",
    "            \n",
    "        elif len(ids) == 4: ## MM, FM, MF, FF\n",
    "            \n",
    "            sent_1_tokens = ar_sents[0].split()\n",
    "            sent_1_label = labels[0]\n",
    "            \n",
    "            sent_2_tokens = ar_sents[1].split()\n",
    "            sent_2_label = labels[1]\n",
    "            \n",
    "            sent_3_tokens = ar_sents[2].split()\n",
    "            sent_3_label = labels[2]\n",
    "            \n",
    "            sent_4_tokens = ar_sents[3].split()\n",
    "            sent_4_label = labels[3]\n",
    "            \n",
    "            # By construction, we know that sentence 1 and sentence 2\n",
    "            # differ in 1st person *AND* sentence 1 and sentence 3\n",
    "            # differ in 2nd person *AND* sentence 2 and 4 differ in 2nd person\n",
    "            # So we can do multiple passes over the 4 sentences to get their tags\n",
    "            \n",
    "\n",
    "#             if ids[0] == \"B-11885\" or ids[0] == \"B-8397\":\n",
    "#                 import pdb; pdb.set_trace()\n",
    "            sent_1_tags_temp, sent_2_tags_temp = tag_four(sent_1_tokens=sent_1_tokens,\n",
    "                                                          sent_1_label=sent_1_label,\n",
    "                                                          sent_1_tags=['B+B'] * len(sent_1_tokens),\n",
    "                                                          sent_2_tokens=sent_2_tokens,\n",
    "                                                          sent_2_label=sent_2_label,\n",
    "                                                          sent_2_tags=['B+B'] * len(sent_2_tokens),\n",
    "                                                          person=1)\n",
    "            \n",
    "            sent_1_gen_map = {i: gen for i, gen in enumerate(sent_1_tags_temp) if gen != 'B+B'}\n",
    "            sent_2_gen_map = {i: gen for i, gen in enumerate(sent_2_tags_temp) if gen != 'B+B'}\n",
    "            \n",
    "\n",
    "            sent_1_tags, sent_3_tags = tag_four(sent_1_tokens=sent_1_tokens,\n",
    "                                                sent_1_label=sent_1_label,\n",
    "                                                sent_1_tags=list(sent_1_tags_temp),\n",
    "                                                sent_2_tokens=sent_3_tokens,\n",
    "                                                sent_2_label=sent_3_label,\n",
    "                                                sent_2_tags=list(sent_1_tags_temp),\n",
    "                                                person=2)\n",
    "\n",
    "            sent_2_tags, sent_4_tags = tag_four(sent_1_tokens=sent_2_tokens,\n",
    "                                                sent_1_label=sent_2_label,\n",
    "                                                sent_1_tags=list(sent_2_tags_temp),\n",
    "                                                sent_2_tokens=sent_4_tokens,\n",
    "                                                sent_2_label=sent_4_label,\n",
    "                                                sent_2_tags=list(sent_2_tags_temp),\n",
    "                                                person=2)\n",
    "\n",
    "#             if ids[0] == 'C-1225':\n",
    "#                 import pdb; pdb.set_trace()\n",
    "#                 print\n",
    "        \n",
    "            for i in sent_1_gen_map:\n",
    "                if sent_1_gen_map[i] != sent_1_tags[i]:\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                    print(ar_sents)\n",
    "                    print()\n",
    "                    \n",
    "#             for i in sent_2_gen_map:\n",
    "#                 if sent_2_gen_map[i] != sent_2_tags[i]:\n",
    "#                     import pdb; pdb.set_trace()\n",
    "#                     print()\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # adding to tokens and tags\n",
    "            tokens.append(sent_1_tokens)\n",
    "            tags.append(sent_1_tags)\n",
    "    \n",
    "            tokens.append(sent_2_tokens)\n",
    "            tags.append(sent_2_tags)\n",
    "            \n",
    "            tokens.append(sent_3_tokens)\n",
    "            tags.append(sent_3_tags)\n",
    "    \n",
    "            tokens.append(sent_4_tokens)\n",
    "            tags.append(sent_4_tags)\n",
    "            \n",
    "            \n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_1_tokens,\n",
    "                                  tags=sent_1_tags,\n",
    "                                  sent_label=sent_1_label,\n",
    "                                  id=ids[0]))\n",
    "\n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_2_tokens,\n",
    "                                  tags=sent_2_tags,\n",
    "                                  sent_label=sent_2_label,\n",
    "                                  id=ids[1]))\n",
    "            \n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_3_tokens,\n",
    "                      tags=sent_3_tags,\n",
    "                      sent_label=sent_3_label,\n",
    "                      id=ids[2]))\n",
    "\n",
    "            tagged_data.append(TokenLevalInfo(tokens=sent_4_tokens,\n",
    "                      tags=sent_4_tags,\n",
    "                      sent_label=sent_4_label,\n",
    "                      id=ids[3]))\n",
    "            \n",
    "            # checking\n",
    "            # number of B's should match accross all 4 sentences\n",
    "            if (sent_1_tags.count('B+B') != sent_2_tags.count('B+B') != sent_3_tags.count('B+B') != sent_4_tags.count('B+B')):\n",
    "                print(ids)\n",
    "                \n",
    "            # checking\n",
    "            first_person_f = (sent_1_tags.count('1F+B') + sent_2_tags.count('1F+B') + \n",
    "                              sent_3_tags.count('1F+B') + sent_4_tags.count('1F+B') +\n",
    "                              sent_1_tags.count('1F+1F') + sent_2_tags.count('1F+1F') + \n",
    "                              sent_3_tags.count('1F+1F') + sent_4_tags.count('1F+1F') +\n",
    "                              sent_1_tags.count('1F+1M') + sent_2_tags.count('1F+1M') + \n",
    "                              sent_3_tags.count('1F+1M') + sent_4_tags.count('1F+1M') +\n",
    "                              sent_1_tags.count('1F+2F') + sent_2_tags.count('1F+2F') + \n",
    "                              sent_3_tags.count('1F+2F') + sent_4_tags.count('1F+2F') +\n",
    "                              sent_1_tags.count('1F+2M') + sent_2_tags.count('1F+2M') + \n",
    "                              sent_3_tags.count('1F+2M') + sent_4_tags.count('1F+2M'))\n",
    "            \n",
    "            first_person_m = (sent_1_tags.count('1M+B') + sent_2_tags.count('1M+B') + \n",
    "                              sent_3_tags.count('1M+B') + sent_4_tags.count('1M+B') +\n",
    "                              sent_1_tags.count('1M+1F') + sent_2_tags.count('1M+1F') +\n",
    "                              sent_3_tags.count('1M+1F') + sent_4_tags.count('1M+1F') +\n",
    "                              sent_1_tags.count('1M+1M') + sent_2_tags.count('1M+1M') +\n",
    "                              sent_3_tags.count('1M+1M') + sent_4_tags.count('1M+1M') +\n",
    "                              sent_1_tags.count('1M+2F') + sent_2_tags.count('1M+2F') +\n",
    "                              sent_3_tags.count('1M+2F') + sent_4_tags.count('1M+2F') +\n",
    "                              sent_1_tags.count('1M+2M') + sent_2_tags.count('1M+2M') +\n",
    "                              sent_3_tags.count('1M+2M') + sent_4_tags.count('1M+2M'))\n",
    "\n",
    "            second_person_f = (sent_1_tags.count('2F+B') + sent_2_tags.count('2F+B') + \n",
    "                               sent_3_tags.count('2F+B') + sent_4_tags.count('2F+B') +\n",
    "                               sent_1_tags.count('2F+1F') + sent_2_tags.count('2F+1F') + \n",
    "                               sent_3_tags.count('2F+1F') + sent_4_tags.count('2F+1F') +\n",
    "                               sent_1_tags.count('2F+1M') + sent_2_tags.count('2F+1M') + \n",
    "                               sent_3_tags.count('2F+1M') + sent_4_tags.count('2F+1M') +\n",
    "                               sent_1_tags.count('2F+2F') + sent_2_tags.count('2F+2F') + \n",
    "                               sent_3_tags.count('2F+2F') + sent_4_tags.count('2F+2F') +\n",
    "                               sent_1_tags.count('2F+2M') + sent_2_tags.count('2F+2M') + \n",
    "                               sent_3_tags.count('2F+2M') + sent_4_tags.count('2F+2M'))\n",
    "\n",
    "            second_person_m = (sent_1_tags.count('2M+B') + sent_2_tags.count('2M+B') + \n",
    "                               sent_3_tags.count('2M+B') + sent_4_tags.count('2M+B') +\n",
    "                               sent_1_tags.count('2M+1F') + sent_2_tags.count('2M+1F') + \n",
    "                               sent_3_tags.count('2M+1F') + sent_4_tags.count('2M+1F') +\n",
    "                               sent_1_tags.count('2M+1M') + sent_2_tags.count('2M+1M') + \n",
    "                               sent_3_tags.count('2M+1M') + sent_4_tags.count('2M+1M')+ \n",
    "                               sent_1_tags.count('2M+2F') + sent_2_tags.count('2M+2F') + \n",
    "                               sent_3_tags.count('2M+2F') + sent_4_tags.count('2M+2F') +\n",
    "                               sent_1_tags.count('2M+2M') + sent_2_tags.count('2M+2M') + \n",
    "                               sent_3_tags.count('2M+2M') + sent_4_tags.count('2M+2M'))\n",
    "            \n",
    "            \n",
    "            clitics_m = (sent_1_tags.count('B+1M') + sent_2_tags.count('B+1M') + \n",
    "                         sent_3_tags.count('B+1M') + sent_4_tags.count('B+1M') +\n",
    "                         sent_1_tags.count('B+2M') + sent_2_tags.count('B+2M') + \n",
    "                         sent_3_tags.count('B+2M') + sent_4_tags.count('B+2M'))\n",
    "            \n",
    "            clitics_f = (sent_1_tags.count('B+1F') + sent_2_tags.count('B+1F') + \n",
    "                         sent_3_tags.count('B+1F') + sent_4_tags.count('B+1F') +\n",
    "                         sent_1_tags.count('B+2F') + sent_2_tags.count('B+2F') + \n",
    "                         sent_3_tags.count('B+2F') + sent_4_tags.count('B+2F'))\n",
    "            \n",
    "            # number of 1F and 1M should match accross all 4 sentences\n",
    "            if first_person_f != first_person_m:\n",
    "                print(f'First Person Problems: {ids}')\n",
    "            \n",
    "            # number of 2F and 1M should match accross all 4 sentences          \n",
    "            if second_person_f != second_person_m:\n",
    "                print(f'Second Person Problems: {ids}')\n",
    "            \n",
    "            if clitics_m != clitics_f:\n",
    "                print(f'Clitics Problems: {ids}')\n",
    "    \n",
    "\n",
    "    return tokens, tags, tagged_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_last(s, replace_what, replace_with):\n",
    "    head, _sep, tail = s.rpartition(replace_what)\n",
    "    return head + replace_with + tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_two(sent_1_tokens, sent_1_label, sent_2_tokens, sent_2_label):\n",
    "    sent_1_tags = []\n",
    "    sent_2_tags = []\n",
    "    \n",
    "    for token_1, token_2 in zip(sent_1_tokens, sent_2_tokens):\n",
    "        clitic_found = False\n",
    "        for clitic_combo in clitics:\n",
    "            clitic_tag_1 = 'B'\n",
    "            clitic_tag_2 = 'B'\n",
    "            # check if the two tokens have clitics or not\n",
    "            # checking if the two word end with clitics\n",
    "            if (token_1.endswith(clitic_combo[0]) and token_2.endswith(clitic_combo[1]) or\n",
    "                token_1.endswith(clitic_combo[1]) and token_2.endswith(clitic_combo[0])):\n",
    "                \n",
    "                token_1_clitic = clitic_combo[0] if token_1.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "                token_2_clitic = clitic_combo[0] if token_2.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "\n",
    "                token_1_base = replace_last(token_1, token_1_clitic, '')\n",
    "                token_2_base = replace_last(token_2, token_2_clitic, '')\n",
    "\n",
    "                # getting the base form gender\n",
    "\n",
    "                token_tag_1, token_tag_2 = compare_two(token_1_base, sent_1_label,\n",
    "                                                       token_2_base, sent_2_label)\n",
    "                \n",
    "                # getting the clitic number\n",
    "                clitic_tag_number = '1' if sent_1_label[0] != 'B' else '2'\n",
    "                \n",
    "                # getting the clitic gender and adding number to it\n",
    "                clitic_tag_1 = clitic_tag_number + clitics_genders[token_1_clitic]\n",
    "                clitic_tag_2 = clitic_tag_number + clitics_genders[token_2_clitic]\n",
    "                \n",
    "                clitic_found = True\n",
    "                break\n",
    "\n",
    "    \n",
    "        # if they don't end clitics, then just get the base form gender\n",
    "        # and tag clitics at B\n",
    "        if clitic_found == False:\n",
    "            token_tag_1, token_tag_2 = compare_two(token_1, sent_1_label, token_2, sent_2_label)\n",
    "\n",
    "\n",
    "        sent_1_tags.append(f'{token_tag_1}+{clitic_tag_1}')\n",
    "        sent_2_tags.append(f'{token_tag_2}+{clitic_tag_2}')\n",
    "    \n",
    "    return sent_1_tags, sent_2_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_four(sent_1_tokens, sent_1_label, sent_1_tags, sent_2_tokens, sent_2_label, sent_2_tags, person=1):\n",
    "\n",
    "\n",
    "    \n",
    "    for i, (token_1, token_2) in enumerate(zip(sent_1_tokens, sent_2_tokens)):\n",
    "        if token_1 != token_2:\n",
    "            \n",
    "            token_tag_1 = sent_1_tags[i].split('+')[0]\n",
    "            token_tag_2 = sent_2_tags[i].split('+')[0]\n",
    "                \n",
    "                \n",
    "            clitic_tag_1 = sent_1_tags[i].split('+')[1]\n",
    "            clitic_tag_2 = sent_2_tags[i].split('+')[1]\n",
    "\n",
    "            if sent_1_tags[i] == 'B+B' and sent_2_tags[i] == 'B+B':\n",
    "                clitic_found = False\n",
    "                for clitic_combo in clitics:\n",
    "\n",
    "                    # check if the two tokens have clitics or not\n",
    "                    # checking if the two word end with clitics\n",
    "                    if (token_1.endswith(clitic_combo[0]) and token_2.endswith(clitic_combo[1]) or\n",
    "                        token_1.endswith(clitic_combo[1]) and token_2.endswith(clitic_combo[0])):\n",
    "\n",
    "                        token_1_clitic = clitic_combo[0] if token_1.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "                        token_2_clitic = clitic_combo[0] if token_2.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "\n",
    "                        token_1_base = replace_last(token_1, token_1_clitic, '')\n",
    "                        token_2_base = replace_last(token_2, token_2_clitic, '')\n",
    "\n",
    "                        # getting the base form gender\n",
    "                        if token_1_base != token_2_base:\n",
    "                            token_tag_1, token_tag_2 = compare_four(token_1_base, sent_1_label,\n",
    "                                                                    token_2_base, sent_2_label,\n",
    "                                                                    person=person)\n",
    "\n",
    "\n",
    "                        # getting the clitic gender\n",
    "                        clitic_tag_1 = str(person) + clitics_genders[token_1_clitic]\n",
    "                        clitic_tag_2 = str(person) + clitics_genders[token_2_clitic]\n",
    "                        clitic_found = True\n",
    "                        break\n",
    "\n",
    "\n",
    "                # if they don't end clitics, then just get the base form gender\n",
    "                # and tag clitics at B\n",
    "                if clitic_found == False:\n",
    "                    token_tag_1, token_tag_2 = compare_four(token_1, sent_1_label, token_2, sent_2_label,\n",
    "                                                            person=person)\n",
    "\n",
    "\n",
    "\n",
    "                sent_1_tags[i] = f'{token_tag_1}+{clitic_tag_1}'\n",
    "                sent_2_tags[i] = f'{token_tag_2}+{clitic_tag_2}'\n",
    "            \n",
    "            \n",
    "            elif sent_1_tags[i] != 'B+B' or sent_2_tags[i] != 'B+B':\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                # if the token has been labeled, then the change must have occured\n",
    "                # either at the base or the clitic or due to a spelling error\n",
    "                \n",
    "                # get the part that changed\n",
    "                clitic_change = True if clitic_tag_1 == clitic_tag_2 == 'B' else False\n",
    "                base_change = True if token_tag_1 == token_tag_2 == 'B' else False\n",
    "                clitic_found = False \n",
    "            \n",
    "                assert clitic_change != base_change\n",
    "                \n",
    "                if clitic_change:\n",
    "                    for clitic_combo in clitics:\n",
    "\n",
    "                        # check if the two tokens have clitics or not\n",
    "                        # checking if the two word end with clitics\n",
    "                        if (token_1.endswith(clitic_combo[0]) and token_2.endswith(clitic_combo[1]) or\n",
    "                            token_1.endswith(clitic_combo[1]) and token_2.endswith(clitic_combo[0])):\n",
    "                            clitic_found = True\n",
    "                            token_1_clitic = clitic_combo[0] if token_1.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "                            token_2_clitic = clitic_combo[0] if token_2.endswith(clitic_combo[0]) else clitic_combo[1]\n",
    "                            \n",
    "                            clitic_tag_1 = str(person) + clitics_genders[token_1_clitic]\n",
    "                            clitic_tag_2 = str(person) + clitics_genders[token_2_clitic]\n",
    "                            clitic_found = True\n",
    "                            break\n",
    "                \n",
    "                \n",
    "                elif base_change:\n",
    "                    for clitic in all_clitics:\n",
    "                        if token_1.endswith(clitic) and token_2.endswith(clitic):\n",
    "                            token_1_base = replace_last(token_1, clitic, '')\n",
    "                            token_2_base = replace_last(token_2, clitic, '')\n",
    "                            token_tag_1, token_tag_2 = compare_four(token_1, sent_1_label, token_2, sent_2_label,\n",
    "                                                                    person=person)\n",
    "                \n",
    "                # no clitic has been found and the change is not in the base,\n",
    "                # then the change must be due to a spelling error\n",
    "                if clitic_found == False and base_change == False:\n",
    "                    import pdb; pdb.set_trace()\n",
    "                sent_1_tags[i] = f'{token_tag_1}+{clitic_tag_1}'\n",
    "                sent_2_tags[i] = f'{token_tag_2}+{clitic_tag_2}'\n",
    "    \n",
    "    return sent_1_tags, sent_2_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_one(token, sent_label, person):\n",
    "    token_tag = None\n",
    "    if sent_label == 'MM' and person == '1':\n",
    "        token_tag = \"1M\"\n",
    "    elif sent_label == 'MM' and person == '2':\n",
    "        token_tag = \"2M\" \n",
    "    \n",
    "    elif sent_label == 'FM' and person == '1':\n",
    "        token_tag = \"1F\"\n",
    "    elif sent_label == 'FM' and person == '2':\n",
    "        token_tag = \"2M\"\n",
    "    \n",
    "    elif sent_label == 'MF' and person == '1':\n",
    "        token_tag = \"1M\"\n",
    "    elif sent_label == 'MF' and person == '2':\n",
    "        token_tag = \"2F\"\n",
    "        \n",
    "    elif sent_label == 'FF' and person == '1':\n",
    "        token_tag = \"1F\"\n",
    "    elif sent_label == 'FF' and person == '2':\n",
    "        token_tag = \"2F\"\n",
    "    \n",
    "    return token_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two(token_1, sent_1_label, token_2, sent_2_label):\n",
    "    token_1_tag = None\n",
    "    token_2_tag = None\n",
    "    if token_1 == token_2:\n",
    "        token_1_tag = 'B'\n",
    "        token_2_tag = 'B'\n",
    "    else:\n",
    "        if sent_1_label == 'FB' and sent_2_label == 'MB':\n",
    "            token_1_tag = '1F'\n",
    "            token_2_tag = '1M'\n",
    "        elif sent_1_label == 'MB' and sent_2_label == 'FB':\n",
    "            token_1_tag = '1M'\n",
    "            token_2_tag = '1F'\n",
    "        if sent_1_label == 'BF' and sent_2_label == 'BM':\n",
    "            token_1_tag = '2F'\n",
    "            token_2_tag = '2M'\n",
    "        elif sent_1_label == 'BM' and sent_2_label == 'BF':\n",
    "            token_1_tag = '2M'\n",
    "            token_2_tag = '2F'\n",
    "        \n",
    "    return token_1_tag, token_2_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_four(token_1, sent_1_label, token_2, sent_2_label, person=1):\n",
    "\n",
    "    token_1_tag = None\n",
    "    token_2_tag = None\n",
    "\n",
    "    # MM case\n",
    "    if sent_1_label == 'MM' and sent_2_label == 'MF':\n",
    "        if person == 1:\n",
    "            token_1_tag = '1M'\n",
    "            token_2_tag = '1M'  \n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    elif sent_1_label == 'MF' and sent_2_label == 'MM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1M'\n",
    "            token_2_tag ='1M'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2M'\n",
    "\n",
    "    elif sent_1_label == 'MM' and sent_2_label == 'FM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1M'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2M'\n",
    "\n",
    "    elif sent_1_label == 'FM' and sent_2_label == 'MM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1M'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2M'\n",
    "\n",
    "    elif sent_1_label == 'MM' and sent_2_label == 'FF':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1M'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    elif sent_1_label == 'FF' and sent_2_label == 'MM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1M'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2M'\n",
    "\n",
    "    # MF case\n",
    "    elif sent_1_label == 'MF' and sent_2_label == 'FM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1M'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2M'\n",
    "\n",
    "    elif sent_1_label == 'FM' and sent_2_label == 'MF':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1M'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    elif sent_1_label == 'MF' and sent_2_label == 'FF':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1M'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    elif sent_1_label == 'FF' and sent_2_label == 'MF':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1M'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    # FM case\n",
    "    elif sent_1_label == 'FM' and sent_2_label == 'FF':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2M'\n",
    "            token_2_tag ='2F'\n",
    "\n",
    "    elif sent_1_label == 'FF' and sent_2_label == 'FM':\n",
    "        if person == 1:\n",
    "            token_1_tag ='1F'\n",
    "            token_2_tag ='1F'\n",
    "        elif person == 2:\n",
    "            token_1_tag ='2F'\n",
    "            token_2_tag ='2M'\n",
    "            \n",
    "    return token_1_tag, token_2_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_balanced_corpora(tagged_input):\n",
    "    target_mm_tagged = []\n",
    "    target_fm_tagged = []\n",
    "    target_mf_tagged = []\n",
    "    target_ff_tagged = []\n",
    "    \n",
    "    for ex in tagged_input:\n",
    "        if ex.sent_label == 'BB':\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "        \n",
    "        elif ex.sent_label == 'MB':\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "        \n",
    "        elif ex.sent_label == 'FB':\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            \n",
    "        elif ex.sent_label == 'BM':\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "        \n",
    "        elif ex.sent_label == 'BF':\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            \n",
    "        elif ex.sent_label == 'MM':\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mm_tagged.append(ex)\n",
    "            target_mm_tagged.append(ex)\n",
    "            \n",
    "        elif ex.sent_label == 'FM':\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            target_fm_tagged.append(ex)\n",
    "            \n",
    "        elif ex.sent_label == 'MF':\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "            target_mf_tagged.append(ex)\n",
    "        \n",
    "        elif ex.sent_label == 'FF':\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "            target_ff_tagged.append(ex)\n",
    "    \n",
    "    assert len(target_mm_tagged) == len(target_fm_tagged) == len(target_mf_tagged) \\\n",
    "            == len(target_ff_tagged) == len(tagged_input)\n",
    "        \n",
    "        \n",
    "    return target_mm_tagged, target_fm_tagged, target_mf_tagged, target_ff_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انا ( ستانلي ) ، مرشدكم الشجاع إلى عالما رائعا من الارتجال', 'انا ( ستانلي ) ، مرشدتكم الشجاعة إلى عالما رائعا من الارتجال', 'انا ( ستانلي ) ، مرشدكن الشجاع إلى عالما رائعا من الارتجال', 'انا ( ستانلي ) ، مرشدتكن الشجاعة إلى عالما رائعا من الارتجال']\n",
      "\n",
      "['و أنا هذا الفتى الذي تريده ؟ هذه غلطة ( كلاهان )', 'و أنا هذه الفتاة التي تريدها ؟ هذه غلطة ( كلاهان )', 'و أنا هذا الفتى الذي تريدينه ؟ هذه غلطة ( كلاهان )', 'و أنا هذه الفتاة التي تريدينها ؟ هذه غلطة ( كلاهان )']\n",
      "\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'B+B': 385693,\n",
       "         '1F+B': 3490,\n",
       "         '1M+B': 3490,\n",
       "         '2M+B': 16320,\n",
       "         '2F+B': 16320,\n",
       "         'B+2M': 1042,\n",
       "         'B+2F': 1042,\n",
       "         '2M+2M': 22,\n",
       "         '2F+2F': 22,\n",
       "         '1M+1M': 9,\n",
       "         '1F+1F': 9,\n",
       "         'B+1M': 28,\n",
       "         'B+1F': 28,\n",
       "         '1M+2M': 1,\n",
       "         '1F+2M': 1,\n",
       "         '1M+2F': 1,\n",
       "         '1F+2F': 1,\n",
       "         '2M+1M': 1,\n",
       "         '2M+1F': 1,\n",
       "         '2F+1M': 1,\n",
       "         '2F+1F': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'train'\n",
    "tokens, tags, tagged_all_data = tag_input_corpus('/Users/ba63/Desktop/repos/apgcv2.0-internal/'\\\n",
    "                                                    'Arabic-parallel-gender-corpus-v-2.0/'+split+'/', split=split)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.arin.tokens.new',\n",
    "           data=tagged_all_data)\n",
    "\n",
    "target_mm_tagged, target_fm_tagged, target_mf_tagged, target_ff_tagged = tag_balanced_corpora(tagged_all_data)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MM.tokens.new',\n",
    "           data=target_mm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FM.tokens.new',\n",
    "           data=target_fm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MF.tokens.new',\n",
    "           data=target_mf_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FF.tokens.new',\n",
    "          data=target_ff_tagged)\n",
    "\n",
    "Counter([x for s in tags for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'B+B': 44629,\n",
       "         '2M+B': 1787,\n",
       "         '2F+B': 1787,\n",
       "         '1M+B': 422,\n",
       "         '1F+B': 422,\n",
       "         'B+2F': 98,\n",
       "         'B+2M': 98,\n",
       "         'B+1F': 5,\n",
       "         'B+1M': 5,\n",
       "         '2M+2M': 2,\n",
       "         '2F+2F': 2})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'dev'\n",
    "tokens, tags, tagged_all_data = tag_input_corpus('/Users/ba63/Desktop/repos/apgcv2.0-internal/'\\\n",
    "                                                    'Arabic-parallel-gender-corpus-v-2.0/'+split+'/', split=split)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.arin.tokens.new',\n",
    "           data=tagged_all_data)\n",
    "\n",
    "target_mm_tagged, target_fm_tagged, target_mf_tagged, target_ff_tagged = tag_balanced_corpora(tagged_all_data)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MM.tokens.new',\n",
    "           data=target_mm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FM.tokens.new',\n",
    "           data=target_fm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MF.tokens.new',\n",
    "           data=target_mf_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FF.tokens.new',\n",
    "          data=target_ff_tagged)\n",
    "\n",
    "Counter([x for s in tags for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'B+B': 108411,\n",
       "         '2M+B': 4548,\n",
       "         '2F+B': 4548,\n",
       "         'B+2M': 279,\n",
       "         'B+2F': 279,\n",
       "         '1M+B': 958,\n",
       "         '1F+B': 958,\n",
       "         '2M+2M': 8,\n",
       "         '2F+2F': 8,\n",
       "         'B+1F': 10,\n",
       "         'B+1M': 10,\n",
       "         '1F+1F': 1,\n",
       "         '1M+1M': 1})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'test'\n",
    "tokens, tags, tagged_all_data = tag_input_corpus('/Users/ba63/Desktop/repos/apgcv2.0-internal/'\\\n",
    "                                                    'Arabic-parallel-gender-corpus-v-2.0/'+split+'/', split=split)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.arin.tokens.new',\n",
    "           data=tagged_all_data)\n",
    "\n",
    "target_mm_tagged, target_fm_tagged, target_mf_tagged, target_ff_tagged = tag_balanced_corpora(tagged_all_data)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MM.tokens.new',\n",
    "           data=target_mm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FM.tokens.new',\n",
    "           data=target_fm_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.MF.tokens.new',\n",
    "           data=target_mf_tagged)\n",
    "\n",
    "write_data(path='new_tokens_data/' + split +'.ar.FF.tokens.new',\n",
    "          data=target_ff_tagged)\n",
    "\n",
    "Counter([x for s in tags for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('camel_8': conda)",
   "language": "python",
   "name": "python38864bitcamel8condac9be0f4ac5324e66b8a8ab27f88e9f35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
