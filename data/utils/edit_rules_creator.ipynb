{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "2fd8ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import re\n",
    "import operator\n",
    "import kenlm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "a5440cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin/lmplz -o 5 < train.char.txt > char.arpa\n",
    "# bin/build_binary char.arpa delme.bin\n",
    "model = kenlm.LanguageModel('/Users/ba63/Desktop/repos/gender-rewriting/kenlm/build/delme.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "6da9d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(words):\n",
    "    scored_words = [(w, model.score(\" \".join(list(w)))) for w in words]\n",
    "    return scored_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "1a38d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        return [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "a917ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedSent:\n",
    "    def __init__(self, id, tokens, tags):\n",
    "        self.id = id\n",
    "        self.tokens = tokens\n",
    "        self.tags = tags\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "\n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "de707e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_stentences(raw_data):\n",
    "    tokenized_sents = []\n",
    "    id, tokens, tags = None, [], []\n",
    "    for line in raw_data:\n",
    "        line = line.split()\n",
    "        if line:\n",
    "            id = line[0]\n",
    "            tokens.append(line[1])\n",
    "            tags.append(line[2])\n",
    "        else:\n",
    "            tokenized_sents.append(TokenizedSent(id, tokens, tags))\n",
    "            id, tokens, tags = None, [], []\n",
    "    \n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "283c13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(src_word, trg_word):\n",
    "    str1 = src_word if len(src_word) <= len(trg_word) else trg_word\n",
    "    str2 = src_word if str1 == trg_word else trg_word\n",
    "    \n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # base cases\n",
    "    for row in range(m):\n",
    "        dp[row][n] = m - row\n",
    "    \n",
    "    for col in range(n):\n",
    "        dp[m][col] = n - col\n",
    "    \n",
    "    # Bottom up dp\n",
    "    for i in range(m - 1, -1, -1):\n",
    "        for j in range(n - 1, -1, -1):\n",
    "            add = 1 if str1[i] != str2[j] else 0\n",
    "            dp[i][j] = min(dp[i + 1][j] + 1, dp[i][j + 1] + 1, dp[i + 1][j + 1] + add)\n",
    "    \n",
    "\n",
    "    return dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "e970cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack_dp(dp, src_word, trg_word):\n",
    "    w1 = src_word if len(src_word) <= len(trg_word) else trg_word\n",
    "    w2 = src_word if w1 == trg_word else trg_word\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    w1_align, w2_align = \"\", \"\"\n",
    "    \n",
    "    while i < len(w1) and j < len(w2):\n",
    "        if dp[i][j] == dp[i][j + 1] + 1:\n",
    "#             print(f'Inserting {w2[j]} in {w1}')\n",
    "            w1_align += '+'\n",
    "            w2_align += w2[j]\n",
    "            j += 1\n",
    "        \n",
    "        elif dp[i][j] == dp[i + 1][j] + 1:\n",
    "#             print(f'Deleting {w1[i]} from {w1}')\n",
    "            w1_align += w1[i]\n",
    "            w2_align += '-'\n",
    "            i += 1\n",
    "        \n",
    "        elif dp[i][j] == dp[i + 1][j + 1]:\n",
    "#             print(f'Copying {w1[i]}')\n",
    "            w1_align += w1[i]\n",
    "            w2_align += w2[j]\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "        elif dp[i][j] == dp[i + 1][j + 1] + 1:\n",
    "#             print(f'Subbing {w1[i]} with {w2[j]}')\n",
    "            w1_align += w1[i]\n",
    "            w2_align += w2[j]\n",
    "            i += 1\n",
    "            j += 1\n",
    "    \n",
    "    assert len(w1_align) <= len(w2_align)\n",
    "    \n",
    "    for k in range(j, len(w2)):\n",
    "#         print(f'inserting {w2[k]} in {w1}')\n",
    "        w1_align += '+'\n",
    "        w2_align += w2[k]\n",
    "    \n",
    "    assert len(w1_align) <= len(w2_align)\n",
    "    \n",
    "    if w1 == src_word and w2 == trg_word:\n",
    "        src_align, trg_align = w1_align, w2_align\n",
    "    \n",
    "    elif w2 == src_word and w1 == trg_word:\n",
    "        src_align, trg_align = w2_align, w1_align\n",
    "\n",
    "    return src_align, trg_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "c5b43ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule(src_align, trg_align):\n",
    "    assert len(src_align) == len(trg_align)\n",
    "    src_pattern = \"\"\n",
    "    trg_pattern = \"\"\n",
    "    for i in range(len(src_align)):\n",
    "        if src_align[i] == trg_align[i]:\n",
    "            src_pattern += 'X'\n",
    "            trg_pattern += 'X'\n",
    "        else:\n",
    "            src_pattern += src_align[i]\n",
    "            trg_pattern += trg_align[i]\n",
    "    return (src_pattern, trg_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "e98aa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_src = read_data('new_tokens_data/train.arin.tokens')\n",
    "train_data_trg_mm = read_data('new_tokens_data/train.ar.MM.tokens')\n",
    "train_data_trg_fm = read_data('new_tokens_data/train.ar.FM.tokens')\n",
    "train_data_trg_mf = read_data('new_tokens_data/train.ar.MF.tokens')\n",
    "train_data_trg_ff = read_data('new_tokens_data/train.ar.FF.tokens')\n",
    "\n",
    "\n",
    "tokenized_sents_src = collate_stentences(train_data_src)\n",
    "tokenized_sents_trg_mm = collate_stentences(train_data_trg_mm)\n",
    "tokenized_sents_trg_mf = collate_stentences(train_data_trg_mf)\n",
    "tokenized_sents_trg_fm = collate_stentences(train_data_trg_fm)\n",
    "tokenized_sents_trg_ff = collate_stentences(train_data_trg_ff)\n",
    "\n",
    "assert len(tokenized_sents_src) == len(tokenized_sents_trg_mm)\n",
    "assert len(tokenized_sents_src) == len(tokenized_sents_trg_fm)\n",
    "assert len(tokenized_sents_src) == len(tokenized_sents_trg_mf)\n",
    "assert len(tokenized_sents_src) == len(tokenized_sents_trg_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "e2b73b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, src_word, trg_word,\n",
    "                 edit_dist, rule):\n",
    "        \n",
    "        self.src_word = src_word\n",
    "        self.trg_word = trg_word\n",
    "        self.edit_dist = edit_dist\n",
    "        self.rule = rule\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "\n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "5a68b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(src_sents, trg_sents):\n",
    "    rules_tag_src_tgt = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "    rules_tag_src = dict()\n",
    "    rules_probs = defaultdict(lambda: defaultdict(lambda: 0)) # P(tgt_pattrn | tgt_tag, src_pattrn)\n",
    "    src_tag_probs = defaultdict(lambda: defaultdict(lambda: 0)) # P(src_pattrn | src_tag)\n",
    "    src_tag_counts_probs = dict() # P(src_tag)\n",
    "                    \n",
    "    for src_sent, trg_sent in zip(src_sents, trg_sents):\n",
    "        for token_1, token_2, tag_1, tag_2 in zip(src_sent.tokens, trg_sent.tokens, src_sent.tags, trg_sent.tags):\n",
    "            if token_1 != token_2:\n",
    "                assert tag_1 != tag_2\n",
    "\n",
    "#                 print(f'Token 1: {token_1}')\n",
    "#                 print(f'Token 2: {token_2}')\n",
    "                edit_distance_table = edit_distance(src_word=token_1, trg_word=token_2)\n",
    "#                 print(f'Edit Dist: {edit_distance_table[0][0]}')\n",
    "#                 print(f'Backtracking:')\n",
    "                \n",
    "                src_align, trg_align = backtrack_dp(edit_distance_table, src_word=token_1, trg_word=token_2)\n",
    "                src_pattern, trg_pattern = get_rule(src_align, trg_align)\n",
    "\n",
    "#                 print(f'Rule: {(src_pattern, trg_pattern)}')\n",
    "#                 print(f'Condensed Rule: {(r1, r2)}')\n",
    "#                 print('================')\n",
    "\n",
    "\n",
    "                rules_tag_src_tgt[(tag_2, src_pattern)][trg_pattern] += 1\n",
    "                rules_tag_src[(tag_2, src_pattern)] = 1 + rules_tag_src.get((tag_2, src_pattern), 0)\n",
    "\n",
    "                src_tag_probs[src_pattern][tag_1] += 1\n",
    "                src_tag_counts_probs[tag_1] = src_tag_counts_probs.get(tag_1, 0) + 1\n",
    "        \n",
    "    \n",
    "    # turning the counts into log probs\n",
    "    for tgt_g, src_pttrn in rules_tag_src_tgt:\n",
    "        for tgt_pttrn in rules_tag_src_tgt[(tgt_g, src_pttrn)]:\n",
    "            rules_probs[(tgt_g, src_pttrn)][tgt_pttrn] = math.log10(rules_tag_src_tgt[(tgt_g, src_pttrn)][tgt_pttrn] / \n",
    "                                                              float(rules_tag_src[(tgt_g, src_pttrn)]))\n",
    "    \n",
    "#     count_src_pttrns = sum([v for k, v in src_pattrn_probs.items()])\n",
    "    for src_pttrn in src_tag_probs:\n",
    "        for src_tag in src_tag_probs[src_pttrn]:\n",
    "            src_tag_probs[src_pttrn][src_tag] = math.log10(src_tag_probs[src_pttrn][src_tag] /\n",
    "                                                              src_tag_counts_probs[src_tag])\n",
    "        \n",
    "    return rules_probs, src_tag_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "251084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_probs_mm, src_pattrn_probs_mm = generate_rules(src_sents=tokenized_sents_src,\n",
    "                                                     trg_sents=tokenized_sents_trg_mm)\n",
    "\n",
    "rules_probs_mf, src_pattrn_probs_mf = generate_rules(src_sents=tokenized_sents_src,\n",
    "                                                     trg_sents=tokenized_sents_trg_mf)\n",
    "\n",
    "rules_probs_fm, src_pattrn_probs_fm = generate_rules(src_sents=tokenized_sents_src,\n",
    "                                                     trg_sents=tokenized_sents_trg_fm)\n",
    "\n",
    "rules_probs_ff, src_pattrn_probs_ff = generate_rules(src_sents=tokenized_sents_src,\n",
    "                                                     trg_sents=tokenized_sents_trg_ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "3fc1b029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Target MM Rules map *****\n",
      "('1M+B', 'XXXXXة') : {'XXXXX+': -0.13199016723600726, 'XXXXXا': -0.5840007013726386, 'XXXXXأ': -2.8344207036815328}\n",
      "defaultdict(<function generate_rules.<locals>.<lambda>.<locals>.<lambda> at 0x7fa51b4baaf0>, {'1F+B': -0.7084047232776474, '2F+B': -1.5332922578057235})\n"
     ]
    }
   ],
   "source": [
    "print('***** Target MM Rules map *****')\n",
    "for key in rules_probs_mm:\n",
    "    trg_gender, src_pattern = key\n",
    "    print(f'{key} : {dict(rules_probs_mm[key])}')\n",
    "    print(src_pattrn_probs_mm[src_pattern])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "550359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_count_mm = sum([len(val) for key, val in rules_probs_mm.items()])\n",
    "rules_count_mf = sum([len(val) for key, val in rules_probs_mf.items()])\n",
    "rules_count_fm = sum([len(val) for key, val in rules_probs_fm.items()])\n",
    "rules_count_ff = sum([len(val) for key, val in rules_probs_ff.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "aafb37a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 889 in the input to target MM RBR model\n",
      "There are 890 in the input to target FM RBR model\n",
      "There are 890 in the input to target MF RBR model\n",
      "There are 892 in the input to target FF RBR model\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {rules_count_mm} in the input to target MM RBR model')\n",
    "print(f'There are {rules_count_mf} in the input to target FM RBR model')\n",
    "print(f'There are {rules_count_fm} in the input to target MF RBR model')\n",
    "print(f'There are {rules_count_ff} in the input to target FF RBR model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "77ffaabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-سوداء\n",
      "أسود++\n"
     ]
    }
   ],
   "source": [
    "str2 = 'أسود'\n",
    "str1 = 'سوداء'\n",
    "dp = edit_distance(src_word=str1, trg_word=str2)\n",
    "\n",
    "src_align, trg_align = backtrack_dp(dp, str1, str2)\n",
    "print('')\n",
    "print(src_align)\n",
    "print(trg_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "2743aae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-XXXاء\n",
      "أXXX++\n"
     ]
    }
   ],
   "source": [
    "src_pattern, trg_pattern= get_rule(src_align, trg_align)\n",
    "print(src_pattern)\n",
    "print(trg_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "6c134aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_rule(word, src_gender, trg_gender, rules_probs, src_pattrn_probs):\n",
    "    \"\"\"\n",
    "    Returns all the rules that match the src token pattern and the target gender\n",
    "    sorted by their frequency\n",
    "    \"\"\"\n",
    "    matched_rules = []\n",
    "    for rule in rules_probs:\n",
    "        target_gender, src_pattern = rule\n",
    "\n",
    "        pattern = src_pattern.replace('+', '').replace('-','').replace('X', '(.)')\n",
    "        \n",
    "        match = re.match(pattern, word)\n",
    "        \n",
    "        # matching on the pattern and the target gender\n",
    "        if match and match[0] == word and trg_gender == target_gender:\n",
    "\n",
    "            matched_rules.append({'src_word': word,\n",
    "                                  'src_pattern': src_pattern, \n",
    "                                  'trg_gender': target_gender,\n",
    "                                  'src_regex': pattern,\n",
    "                                  'targets': dict(rules_probs[rule]),\n",
    "                                  'src_pattern_prob': src_pattrn_probs[src_pattern][src_gender]})\n",
    "        \n",
    "        \n",
    "    return sorted(matched_rules, key=lambda x: x['src_pattern_prob'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "748dca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(target_rule, src_word, src_pattern):\n",
    "    \"\"\"\n",
    "    Generates a token given a src word, src pattern and target pattern\n",
    "    \"\"\"\n",
    "    tgt_pattern = target_rule.replace('+', '').replace('-','')\n",
    "    x_count = 1\n",
    "    while 'X' in tgt_pattern:\n",
    "        tgt_pattern = tgt_pattern.replace('X', f'\\\\{x_count}', 1)\n",
    "        x_count += 1\n",
    "    # generate target words\n",
    "    trg_word = re.sub(src_pattern, tgt_pattern, src_word)\n",
    "    return trg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "d294875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_rule(matched_rules, pick_top_overall_rule=False, pick_top_target_rule=False):\n",
    "    \"\"\"\n",
    "    Returns generated token(s) given the matched rules.\n",
    "    \n",
    "    Note: we have two forms for rules to pick from:\n",
    "          1) pick_top_overall_rule: Selects the (src_pattern, trg_gender) rule that\n",
    "             occured the most in the training data\n",
    "          2) pick_top_target_rule: Selects the target_pattern that appeared the most\n",
    "             for a given (src_pattern, trg_gender). Because one (src_pattern, trg_gender)\n",
    "             could have multiple target patterns\n",
    "            \n",
    "          We also do not apply any pattern that appeared only 1 during training to reduce\n",
    "          noisy outputs.\n",
    "    \"\"\"\n",
    "    generated_tokens = dict()\n",
    "    if pick_top_overall_rule:\n",
    "        matched_rule = max(matched_rules, key=lambda x: x['src_pattern_prob'])\n",
    "        if pick_top_target_rule:\n",
    "            tgt_rule, tgt_rule_prob = max(matched_rule['targets'].items(), key=lambda x: x[1])\n",
    "\n",
    "            generated_token = generate_token(tgt_rule, matched_rule['src_word'],\n",
    "                                             matched_rule['src_regex'])\n",
    "\n",
    "            log_prob_kenlm = model.score(\" \".join(generated_token))\n",
    "\n",
    "            generated_tokens[generated_token] = log_prob_kenlm + tgt_rule_prob + matched_rule['src_pattern_prob']\n",
    "\n",
    "        else:\n",
    "            generated_tokens = []\n",
    "\n",
    "            for tgt_rule, tgt_rule_prob in matched_rule['targets'].items():\n",
    "\n",
    "                generated_token = generate_token(tgt_rule, matched_rule['src_word'],\n",
    "                                                 matched_rule['src_regex'])\n",
    "\n",
    "                log_prob_kenlm = model.score(\" \".join(generated_token))\n",
    "\n",
    "                generated_tokens[generated_token] = log_prob_kenlm + tgt_rule_prob + matched_rule['src_pattern_prob']\n",
    "\n",
    "    else:\n",
    "        for matched_rule in matched_rules:\n",
    "            if pick_top_target_rule:\n",
    "                tgt_rule, tgt_rule_prob = max(matched_rule['targets'].items(), key=lambda x: x[1])\n",
    "\n",
    "                generated_token = generate_token(tgt_rule, matched_rule['src_word'],\n",
    "                                                 matched_rule['src_regex'])\n",
    "\n",
    "                log_prob_kenlm = model.score(\" \".join(generated_token))\n",
    "\n",
    "                generated_tokens[generated_token] = log_prob_kenlm + tgt_rule_prob + matched_rule['src_pattern_prob']\n",
    "\n",
    "            else:\n",
    "                for tgt_rule, tgt_rule_prob in matched_rule['targets'].items():\n",
    "\n",
    "                    generated_token = generate_token(tgt_rule, matched_rule['src_word'],\n",
    "                                                     matched_rule['src_regex'])\n",
    "                \n",
    "                    log_prob_kenlm = model.score(\" \".join(generated_token))\n",
    "\n",
    "                    generated_tokens[generated_token] = log_prob_kenlm + tgt_rule_prob + matched_rule['src_pattern_prob']\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c183bf",
   "metadata": {},
   "source": [
    "P(tgt_pattern | src_pattern, tgt_gender)\n",
    "P(src_pattern)\n",
    "\n",
    "log P(src_pattern) + log P(tgt_pattern | src_pattern, tgt_gender) + log p_lm(tgt_word)\n",
    "\n",
    "P(src_pattern | src_gender)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "df66a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXXXة',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)ة',\n",
       "  'targets': {'XXXX+': -0.10905672430265556,\n",
       "   'XXXXا': -0.6618986929604364,\n",
       "   'XXXXه': -2.5483894181329183,\n",
       "   'XXXXي': -2.8494194137968996},\n",
       "  'src_pattern_prob': -1.363300740620943},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXيXX',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)ي(.)(.)',\n",
       "  'targets': {'XX+XX': -0.01639041618816937, 'XXاXX': -1.4313637641589874},\n",
       "  'src_pattern_prob': -1.9362583502445982},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXXX+X',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXXوX': -0.025235823416073402,\n",
       "   'XXXXتX': -1.4913616938342726,\n",
       "   'XXXXيX': -1.6163004304425725},\n",
       "  'src_pattern_prob': -2.1192984692556074},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXXXX+',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXXXة': -0.36797678529459443,\n",
       "   'XXXXXم': -0.5440680443502757,\n",
       "   'XXXXXي': -0.5440680443502757},\n",
       "  'src_pattern_prob': -3.3676221144035856},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXX+XX',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXوXX': 0.0},\n",
       "  'src_pattern_prob': -3.73559889969818},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXXXX++',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXXXوا': -0.3010299956639812, 'XXXXXين': -0.3010299956639812},\n",
       "  'src_pattern_prob': -3.9116901587538613},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XX+XXX',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXأXXX': 0.0},\n",
       "  'src_pattern_prob': -3.9116901587538613},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XX+X+XX',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXيXوXX': 0.0},\n",
       "  'src_pattern_prob': -3.9116901587538613},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXX+X+X',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXيXوX': 0.0},\n",
       "  'src_pattern_prob': -4.212720154417842},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXيXة',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)ي(.)ة',\n",
       "  'targets': {'XX+X+': 0.0},\n",
       "  'src_pattern_prob': -4.212720154417842},\n",
       " {'src_word': 'صديقة',\n",
       "  'src_pattern': 'XXXX++X',\n",
       "  'trg_gender': '2M+B',\n",
       "  'src_regex': '(.)(.)(.)(.)(.)',\n",
       "  'targets': {'XXXXنوX': 0.0},\n",
       "  'src_pattern_prob': -4.212720154417842}]"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_rules = match_rule(word='صديقة',\n",
    "                           src_gender='2F+B',\n",
    "                           trg_gender='2M+B',\n",
    "                           rules_probs=rules_probs_mm,\n",
    "                           src_pattrn_probs=src_pattrn_probs_mm)\n",
    "matched_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "89657d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('صديق', -6.961509896411636),\n",
       " ('صدقة', -8.274114735518461),\n",
       " ('صديقه', -9.37475912085103),\n",
       " ('صدق', -9.378628014220087),\n",
       " ('صديقا', -9.412446995989827),\n",
       " ('صديقي', -9.524039028776974),\n",
       " ('صديقوة', -10.249052229378224),\n",
       " ('صديقية', -11.195691444253844),\n",
       " ('صديقتة', -11.202252474949011),\n",
       " ('صداقة', -11.468139387352805),\n",
       " ('صديوقة', -14.85277304825775),\n",
       " ('صديقنوة', -16.066170929564326),\n",
       " ('صديقةة', -16.707800247049253),\n",
       " ('صدأيقة', -16.917578143983352),\n",
       " ('صديقةي', -17.155691547303665),\n",
       " ('صدييوقة', -17.275850696473587),\n",
       " ('صدييقوة', -17.372599838866083),\n",
       " ('صديقةم', -17.556175632386672),\n",
       " ('صديقةين', -18.492648361815302),\n",
       " ('صديقةوا', -18.614831207884638)]"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if matched_rules:\n",
    "    generated_tokens = generate_using_rule(matched_rules, pick_top_overall_rule=False,\n",
    "                                           pick_top_target_rule=False)\n",
    "\n",
    "    \n",
    "sorted(generated_tokens.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "f809b4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('صديق', -6.961509896411636),\n",
       " ('صدقة', -8.274114735518461),\n",
       " ('صدق', -9.378628014220087),\n",
       " ('صديقوة', -10.249052229378224),\n",
       " ('صديوقة', -14.85277304825775),\n",
       " ('صديقنوة', -16.066170929564326),\n",
       " ('صديقةة', -16.707800247049253),\n",
       " ('صدأيقة', -16.917578143983352),\n",
       " ('صدييوقة', -17.275850696473587),\n",
       " ('صدييقوة', -17.372599838866083),\n",
       " ('صديقةوا', -18.614831207884638)]"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if matched_rules:\n",
    "    generated_tokens = generate_using_rule(matched_rules, pick_top_overall_rule=False,\n",
    "                                           pick_top_target_rule=True)\n",
    "\n",
    "    \n",
    "sorted(generated_tokens.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0c771",
   "metadata": {},
   "source": [
    "#### Getting stats on the frequency of the rules\n",
    "#### How many rules appeared only once, twice, 3 times, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e8da7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(rules):\n",
    "    # how many rules appeard once, twice, 3 times,...?\n",
    "    rules_by_count = defaultdict(list)\n",
    "    for key, val in rules.items():\n",
    "        for rule, count in val.items():\n",
    "            rules_by_count[count].append((key, rule))\n",
    "    \n",
    "    rules_by_count_lengths = {k: len(v) for k, v in rules_by_count.items()}\n",
    "    rules_by_count_lengths_sorted = sorted(rules_by_count_lengths, key=rules_by_count_lengths.get, reverse=True)\n",
    "    for k in rules_by_count_lengths_sorted:\n",
    "        print(f'{rules_by_count_lengths[k]} rule(s) appeared {k} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea88950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 rule(s) appeared 1 times\n",
      "120 rule(s) appeared 2 times\n",
      "60 rule(s) appeared 4 times\n",
      "59 rule(s) appeared 3 times\n",
      "41 rule(s) appeared 5 times\n",
      "13 rule(s) appeared 12 times\n",
      "13 rule(s) appeared 8 times\n",
      "13 rule(s) appeared 11 times\n",
      "12 rule(s) appeared 9 times\n",
      "10 rule(s) appeared 7 times\n",
      "9 rule(s) appeared 13 times\n",
      "7 rule(s) appeared 14 times\n",
      "7 rule(s) appeared 6 times\n",
      "6 rule(s) appeared 10 times\n",
      "6 rule(s) appeared 18 times\n",
      "5 rule(s) appeared 20 times\n",
      "4 rule(s) appeared 17 times\n",
      "4 rule(s) appeared 26 times\n",
      "4 rule(s) appeared 15 times\n",
      "3 rule(s) appeared 37 times\n",
      "3 rule(s) appeared 44 times\n",
      "2 rule(s) appeared 51 times\n",
      "2 rule(s) appeared 28 times\n",
      "2 rule(s) appeared 57 times\n",
      "2 rule(s) appeared 19 times\n",
      "2 rule(s) appeared 143 times\n",
      "2 rule(s) appeared 22 times\n",
      "2 rule(s) appeared 63 times\n",
      "2 rule(s) appeared 93 times\n",
      "2 rule(s) appeared 21 times\n",
      "2 rule(s) appeared 29 times\n",
      "1 rule(s) appeared 504 times\n",
      "1 rule(s) appeared 178 times\n",
      "1 rule(s) appeared 850 times\n",
      "1 rule(s) appeared 373 times\n",
      "1 rule(s) appeared 271 times\n",
      "1 rule(s) appeared 404 times\n",
      "1 rule(s) appeared 73 times\n",
      "1 rule(s) appeared 2314 times\n",
      "1 rule(s) appeared 339 times\n",
      "1 rule(s) appeared 84 times\n",
      "1 rule(s) appeared 617 times\n",
      "1 rule(s) appeared 158 times\n",
      "1 rule(s) appeared 1097 times\n",
      "1 rule(s) appeared 190 times\n",
      "1 rule(s) appeared 75 times\n",
      "1 rule(s) appeared 24 times\n",
      "1 rule(s) appeared 694 times\n",
      "1 rule(s) appeared 166 times\n",
      "1 rule(s) appeared 47 times\n",
      "1 rule(s) appeared 67 times\n",
      "1 rule(s) appeared 2097 times\n",
      "1 rule(s) appeared 36 times\n",
      "1 rule(s) appeared 46 times\n",
      "1 rule(s) appeared 115 times\n",
      "1 rule(s) appeared 338 times\n",
      "1 rule(s) appeared 52 times\n",
      "1 rule(s) appeared 125 times\n",
      "1 rule(s) appeared 544 times\n",
      "1 rule(s) appeared 89 times\n",
      "1 rule(s) appeared 117 times\n",
      "1 rule(s) appeared 550 times\n",
      "1 rule(s) appeared 154 times\n",
      "1 rule(s) appeared 326 times\n",
      "1 rule(s) appeared 49 times\n",
      "1 rule(s) appeared 149 times\n",
      "1 rule(s) appeared 134 times\n",
      "1 rule(s) appeared 70 times\n",
      "1 rule(s) appeared 325 times\n",
      "1 rule(s) appeared 172 times\n",
      "1 rule(s) appeared 324 times\n",
      "1 rule(s) appeared 203 times\n",
      "1 rule(s) appeared 126 times\n",
      "1 rule(s) appeared 300 times\n",
      "1 rule(s) appeared 146 times\n",
      "1 rule(s) appeared 297 times\n",
      "1 rule(s) appeared 163 times\n",
      "1 rule(s) appeared 45 times\n",
      "1 rule(s) appeared 161 times\n",
      "1 rule(s) appeared 148 times\n",
      "1 rule(s) appeared 182 times\n",
      "1 rule(s) appeared 76 times\n",
      "1 rule(s) appeared 40 times\n",
      "1 rule(s) appeared 95 times\n",
      "1 rule(s) appeared 54 times\n",
      "1 rule(s) appeared 118 times\n",
      "1 rule(s) appeared 94 times\n",
      "1 rule(s) appeared 34 times\n",
      "1 rule(s) appeared 55 times\n",
      "1 rule(s) appeared 53 times\n",
      "1 rule(s) appeared 88 times\n",
      "1 rule(s) appeared 58 times\n",
      "1 rule(s) appeared 41 times\n",
      "1 rule(s) appeared 31 times\n",
      "1 rule(s) appeared 16 times\n",
      "1 rule(s) appeared 25 times\n",
      "1 rule(s) appeared 32 times\n"
     ]
    }
   ],
   "source": [
    "get_stats(new_rules_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e49d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694f3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('camel_8': conda)",
   "language": "python",
   "name": "python38864bitcamel8condac9be0f4ac5324e66b8a8ab27f88e9f35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
