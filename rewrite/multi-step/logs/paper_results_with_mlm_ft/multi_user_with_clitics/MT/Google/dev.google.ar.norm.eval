Evaluation Against /scratch/ba63/Arabic-Parallel-Gender-Corpus/m2_edits/v2.0/norm_data/MT//dev.ar.MM.norm:
{
 "name": "BLEU",
 "score": 13.8,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "40.3/19.0/9.7/4.9 (BP = 1.000 ratio = 1.110 hyp_len = 54671 ref_len = 49257)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
Evaluation Against /scratch/ba63/Arabic-Parallel-Gender-Corpus/m2_edits/v2.0/norm_data/MT//dev.ar.FM.norm:
{
 "name": "BLEU",
 "score": 13.3,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "39.7/18.4/9.3/4.6 (BP = 1.000 ratio = 1.110 hyp_len = 54671 ref_len = 49257)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
Evaluation Against /scratch/ba63/Arabic-Parallel-Gender-Corpus/m2_edits/v2.0/norm_data/MT//dev.ar.MF.norm:
{
 "name": "BLEU",
 "score": 11.7,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "38.0/16.5/7.9/3.7 (BP = 1.000 ratio = 1.110 hyp_len = 54671 ref_len = 49257)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
Evaluation Against /scratch/ba63/Arabic-Parallel-Gender-Corpus/m2_edits/v2.0/norm_data/MT//dev.ar.FF.norm:
{
 "name": "BLEU",
 "score": 11.3,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "37.5/16.0/7.5/3.6 (BP = 1.000 ratio = 1.110 hyp_len = 54671 ref_len = 49257)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}

Multi-Reference Evaluation
{
 "name": "BLEU",
 "score": 13.9,
 "signature": "nrefs:4|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "40.5/19.2/9.8/4.9 (BP = 1.000 ratio = 1.110 hyp_len = 54671 ref_len = 49257)",
 "nrefs": "4",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
